{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Pandas.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asantucci/Python-Workshop/blob/main/3_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajFD2tfFsdYe"
      },
      "source": [
        "# Pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z4cZIIOyywG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install --upgrade plotly  # <-- The leading prefix indicates this is actually a shell command. This package will be imported later..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCrH6K40KivP"
      },
      "source": [
        "# A simple read_csv example.\n",
        "# (Note that this file only contains name and email...it's not intended to be a replica of what we created above...)\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/names.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KcMcq0RQbdD"
      },
      "source": [
        "# Covid-19 Analysis !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPLvVuaDQfHY"
      },
      "source": [
        "# Ask pandas to read in a Comma Separated Values dataset (i.e. a spreadsheet) from the following URL,\n",
        "# where we additionally specify that the first column should be treated as a Date object.\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\",parse_dates=[0])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoItxDhcQzVt"
      },
      "source": [
        "# The `pivot` operation basically reshapes the data. So here, we're saying:\n",
        "# Use the `date` variable as the row-index or unit-of-observation...\n",
        "# ...and take each of the values in the `state` column and turn them into separate\n",
        "# \"features\" (or columns themselves). What values should go in these columns?\n",
        "# Precisely those specified by the argument which dictates that the number of cases\n",
        "# should be used.\n",
        "# Lastly, we replace NA's (Not Available, i.e. missing data) with Zeros.\n",
        "cases_states = df.pivot(index='date',columns='state',values='cases')\n",
        "cases_states = cases_states.fillna(0)\n",
        "print(cases_states)  # Each row is a day, each column is a state. Values are number of cases.\n",
        "# Here, we request to plot just a few columns.\n",
        "cases_states.plot(y=['California','New York','Florida'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVQiiw84Q5MF"
      },
      "source": [
        "# The `diff()` function effectively takes the value in the current row and subtracts\n",
        "# from it the value in the preceding row. I.e. we calculate the incremental difference\n",
        "# in number of cases, by day, for each state.\n",
        "daily_cases_states = cases_states.diff()\n",
        "# Note that naturally the first observation(s) for each state will be missing\n",
        "# (since there is no preceding value to use), and so again we fill with zeros.\n",
        "daily_cases_states = daily_cases_states.fillna(0)\n",
        "print(daily_cases_states)\n",
        "# Here we plot the deltas.\n",
        "daily_cases_states.plot(y=['California','New York','Florida'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eOnS4QHmzb4"
      },
      "source": [
        "# Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVQrEKEXnGbc"
      },
      "source": [
        "# Build a DF from a dictionary\n",
        "data = {\n",
        "    'Name':['Leo', 'Bob', 'John'],\n",
        "    'WakeupTime':[pd.Timestamp('07:00:00'), pd.Timestamp('08:30:00'), pd.Timestamp('07:30:00')],\n",
        "    'GPA':np.arange(1.0, 4.0),\n",
        "    'School':'Stanford',\n",
        "    'Siblings':np.array([1, 2, 0]),    \n",
        "}\n",
        "df = pd.DataFrame(data) \n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll03kqLtfRZL"
      },
      "source": [
        "## Renaming columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e63JK8SfQCj"
      },
      "source": [
        "df = df.rename(columns = {'WakeupTime' : 'wakeup_time', 'GPA' : 'grade_point_average'})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRCKQ4SgYy0"
      },
      "source": [
        "## Transforming data using a function or mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WdKwLOSgYgu"
      },
      "source": [
        "df[\"ineq\"] = df[\"grade_point_average\"].le(df.Siblings)  # There are plenty of builtin pandas operators!\n",
        "df[\"cumsum\"] = df.grade_point_average.cumsum()          # Before implementing a function, check for built-in methods.\n",
        "df[\"half\"] = df.grade_point_average.apply(lambda x : x / 2.0)  # You can of course also use your own functions...\n",
        "print(df[[\"grade_point_average\", \"Siblings\", \"ineq\", \"cumsum\", \"half\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9G00dkoiH-Z"
      },
      "source": [
        "## Removal of duplicates in a data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3DXLoxbiLJX"
      },
      "source": [
        "df = pd.DataFrame({\"first\" : list(string.ascii_lowercase)[:5] * 2, \"last\" : \"homogeneous\"})\n",
        "# Here, we've created a data.frame where all the last names are the same.\n",
        "# The first names are repeated twice.\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjgFTRHhjoQo"
      },
      "source": [
        "df = df.drop_duplicates()\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOsPFmPLkJq0"
      },
      "source": [
        "#@title Exercise\n",
        "# Extract unique rows of the following data.frame when considering columns 'a', 'b'.\n",
        "df = pd.DataFrame({'a' : list(string.ascii_lowercase[:5]) * 2,\n",
        "                   'b' : list(string.ascii_lowercase[:5]) * 2,\n",
        "                   'c' : list(range(5*2))})\n",
        "print(\"By construction, columns `a` and `b` are identical.\\n\", df)\n",
        "\n",
        "print(\"If we just asked for unique rows, notice that column c has unique values...we'd return the whole DataFrame:\\n\",\n",
        "      df.drop_duplicates())\n",
        "\n",
        "print(\"But, we can also say, 'Only look at certain columns when identifying duplicates', e.g. columns 'a', 'b'\\n\",\n",
        "      df.drop_duplicates(subset = ['a', 'b']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS_K-WMKeuh7"
      },
      "source": [
        "## Indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra-CcizP10Dw"
      },
      "source": [
        "# Make a column the index (and implicitly remove it as a variable, since that would be redundant).\n",
        "df2 = df.set_index('Name')\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNpJwAwxPFG"
      },
      "source": [
        "### Exercice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltvJlDQYxXAC"
      },
      "source": [
        "# Build the dataframe where *names are the indices*\n",
        "# and email & age are the columns. Adjust column names accordingly.\n",
        "names  = ['Leo', 'Bob', 'Jess', 'Casey', 'John', 'Cherr']\n",
        "emails = ['lc@comp.com', 'bob@stanford.edu', 'j@e.ss', 'casey@my.me', 'john@deer.us', 'cherr@y.net']\n",
        "ages   = np.random.randint(1, 30, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSMfEIppbWMy"
      },
      "source": [
        "#@title Solution\n",
        "# Note that here, we provide columns and index separately.\n",
        "df = pd.DataFrame({'Email':emails, 'Age':ages}, index=names)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht2ImpMyZexn"
      },
      "source": [
        "## Join (i.e. merge)\n",
        "It's very natural to want to join to data-frames together. We must have a common key in order to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9QKObeWcrvO"
      },
      "source": [
        "list(string.ascii_lowercase)[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJmqZCv-Zdur"
      },
      "source": [
        "# As an academic example, let's examine some Census data for names in the US.\n",
        "import string\n",
        "males = pd.read_csv(\"https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/dist.male.first\", delimiter=r\"\\s+\", header = None, names = ['First', 'a', 'b', 'c'])\n",
        "females = pd.read_csv(\"https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/dist.female.first\", delimiter=r\"\\s+\", header = None, names = ['First', 'x', 'y', 'z'])\n",
        "\n",
        "print(males)\n",
        "print(females)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FigO0-5anUF"
      },
      "source": [
        "# We *implicitly* merge on the variable(s) that is (are) common to both tables.\n",
        "males.merge(females, how = \"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqVLJ_CEdcJp"
      },
      "source": [
        "### Types of Joins\n",
        "You may be curious about different types of joins. The default argument for `merge.()` is `how = \"inner\"` which means to take the *intersection* between the two tables when using the specified key (which is implicitly taken to be the variable that is common to both tables).\n",
        "\n",
        "If you specify `how = \"left\"`, it means \"keep all of the rows that appear on the left hand side of the merge, and add the columns for the rows that match on the keys found on the right hand side of the merge. We can similarly reason about `how = \"right\"`.\n",
        "\n",
        "Lastly, specifying `how = \"outter\"` means \"keep all of the rows, i.e. the *union* of the two tables when using the specified key as argument.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg-9oBgIpGiQ"
      },
      "source": [
        "## DataFrame Concatenation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR_V5CSIpF3H"
      },
      "source": [
        "# Sometimes, it's handy to row-bind two data-frames together.\n",
        "names = pd.concat([males, females])\n",
        "print(names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK48jcAmo0Oo"
      },
      "source": [
        "## Analyzing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGRKY50SL2f"
      },
      "source": [
        "# Create some data\n",
        "months = pd.date_range(start='20190101', periods=12, freq='M')    # Handy utility for creating a sequence of dates. Here, we ask for 12-months of data.\n",
        "change = np.random.normal(0, 1.2, (12, 3))                        # Draw from a normal distribution, mean=0 and std_dev=1.2, 12x3 matrix.\n",
        "stocks = ['GOOG', 'TSLA', 'APPL']                                 # 3 different stock tickers.\n",
        "df = pd.DataFrame(change, index=months, columns=stocks)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJc0nwMESxMT"
      },
      "source": [
        "Quick glance at data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SipfFHNo2M-"
      },
      "source": [
        "print(df.head(3), '\\n')\n",
        "print(df.tail(2), '\\n')\n",
        "print(df.describe(), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afEb_i4iSykE"
      },
      "source": [
        "### Indices and \"Selecting\" (or extracting) data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvla0LCAqkFS"
      },
      "source": [
        "## Selection using labels\n",
        "\n",
        "# One columns\n",
        "print(\"Extracting a column using a string to describe the column-name\\n\", df['GOOG'], '\\n')\n",
        "\n",
        "# A slice of rows\n",
        "print(\"Here we request for several rows and all column alongside them...\\n\", df[2:5], '\\n')\n",
        "\n",
        "# Multiple rows & columns\n",
        "# Endpoints INCLUDED, unlike in regular Python slicing syntax\n",
        "print(\"Notice that the slicing works (arguably) slightly different here (endpoints _included_)...\\n\", \n",
        "      df.loc['2019-07-31':'2019-09-30',['TSLA','GOOG']], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiMIRdN-sRsF"
      },
      "source": [
        "## Selection using conditions\n",
        "\n",
        "print(df, '\\n')\n",
        "print(df.loc[df['GOOG'] > 2.5,:], '\\n')         # Extracts some rows, all columns.\n",
        "print(df.loc[df.index >= '2019-08-15',:], '\\n') # Extracts some rows, all columns.\n",
        "print(df[df > 0.5], '\\n')                       # All data (i.e. perform the comparison element-wise for every element in the data-frame!)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqT5-0zB-7Pk"
      },
      "source": [
        "### Groupby"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbXhaAA2-9zq"
      },
      "source": [
        "data = {'Name': ['Tom\\'s Pizza', 'Leo\\'s Taqueria', 'John\\'s Burgers', 'Cindy\\'s Peluqueria', 'Sergio\\'s Tacos', 'Bazyli\\'s Pub'],\n",
        "        'Location':['NYC','SF','WDC','SF','SF','NYC'],\n",
        "        'Num Customers':[5, 3, 8, 4, 6, 8],\n",
        "        'Revenue':[32.6, 54.6, 43.8, 43.6, 32.6, 97.5]}\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF0VI2bfEPk"
      },
      "source": [
        "groups = df.groupby('Location')\n",
        "# The iteration style is as follows:\n",
        "# For each \"name\" and \"group\" within the set of \"groups\"\n",
        "for n,g in groups:\n",
        "    print(\"-------\\nGroup {}\\n\".format(n))\n",
        "    print(g.mean())\n",
        "\n",
        "# A similar operation could be done in one line...\n",
        "# df2 = df.groupby('Location').mean()\n",
        "df2 = groups.mean()\n",
        "print(\"\\nHere we print the means for each group but the results are in a data-frame...\\n\", df2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR38BuCOzFk4"
      },
      "source": [
        "## Pivot (i.e. reshape long --> wide)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeRJacxizUPK"
      },
      "source": [
        "df = pd.DataFrame({'date'    :['2020-01-01', '2020-01-01', '2020-02-01', '2020-02-01'],\n",
        "                   'crypto'  :['BTC',        'ETH',        'ETH',        'BTC'],\n",
        "                   'price'   :['8192',       '350',        '405',        '9510'],\n",
        "                   'exchange':['Coinbase',   'Bitconnect', 'Bitconnect', 'Bitconnect']})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BTZ8D93NKu"
      },
      "source": [
        "# Reshape the data...from long --> wide.\n",
        "# Each observation will be a date. The columns *names* will be described by the values in the\n",
        "# \"crypto\" column, but these variables/new-features will take on *values* as described\n",
        "# by the \"price\" column.\n",
        "df2 = df.pivot(index='date',columns='crypto',values='price')\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiMPCn0TX-Lt"
      },
      "source": [
        "## Melt (i.e. reshape wide --> long)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVNfWydXekt"
      },
      "source": [
        "# You might be wondering about going the other direction...\n",
        "# I.e. can we take a data.frame and go from wide --> long?\n",
        "# Of course! We use the method `melt`. If we wish to use the index as the\n",
        "# id-variable, we don't even need to supply any arguments to the method-call!\n",
        "df2.melt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlOnExA-KTZ1"
      },
      "source": [
        "# Note that the \"parse_dates\" arg simply expects a list of indices describing columns\n",
        "# which should be parsed as dates. The indices are zero-indexed, of course! :)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/stocks.csv',parse_dates=[1])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1ywX_rRR7rN"
      },
      "source": [
        "# We do a little reshaping.\n",
        "# Set each row to be a unique date. The columns will take on names described\n",
        "# by the set of values in the \"Stock\" column, and the values themselves will\n",
        "# be filled in using the \"Open\" price.\n",
        "df2 = df.pivot(index='Date',columns='Stock',values='Open')\n",
        "print(df2.head)\n",
        "# Here, we simply plot out the opening price on each day for a couple companies...\n",
        "df2.plot(y=['APPL','SBUX'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtQ_7uiOmmvz"
      },
      "source": [
        "## Sampling rows\n",
        "This is commonly done in statistics and ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNhJmmHDmrdn"
      },
      "source": [
        "print(\"Our df2 originally had \", df2.shape[0], \" rows.\\n\")\n",
        "df2.sample(frac = 1/2)  # Can also specify N explicitly. Note that no modication is done in place"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLiTSQwYoOrz"
      },
      "source": [
        "### Exercise\n",
        "Figure out how to sample 5 rows with replacement from `df2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aM4U2DAnlq3"
      },
      "source": [
        "#@title Solution\n",
        "# We can use arguments to the sample method.\n",
        "import random                      # This import statement not really required...\n",
        "random.seed(1)                     # ...we just use it to set a \"seed\" and reproduce random results.\n",
        "df2.sample(n = 5, replace = True)  # <-- Notice that the first and last row are the same (i.e. it got resampled!)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6l_ZRkUv85"
      },
      "source": [
        "## More on Indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWXci9NHUV9X"
      },
      "source": [
        "# You may be wondering if it's possible to set a multi index.\n",
        "# Of course this is do-able! You simply pass a list of indices.\n",
        "names  = ['Leo', 'Bob', 'Jess', 'Casey', 'John', 'Cherr']\n",
        "emails = ['lc@comp.com', 'bob@stanford.edu', 'j@e.ss', 'casey@my.me', 'john@deer.us', 'cherr@y.net']\n",
        "ages   = np.random.randint(1, 30, 6)\n",
        "df = pd.DataFrame({'Age' : ages}, index = [names, emails])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO7TlCReWXZU"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNY233rHWUWi"
      },
      "source": [
        "# Index into the first row of the data-frame using the `loc` method for a pandas\n",
        "# data.frame, and don't simply use a row-number! Instead, use the index (in some way)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6yDNQHxUU5cL"
      },
      "source": [
        "#@title Solution\n",
        "# You can use a tuple to index into a data-frame (if it has a multi-index).\n",
        "df.loc[(\"Leo\", \"lc@comp.com\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrg_VaEQW_E"
      },
      "source": [
        "# Fancy Plotting! (More motivation than anything)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqpKUbBQlql"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import plotly.express as px\n",
        "import json\n",
        "\n",
        "# The \"with\" keyword is just syntactic sugar. We're simply opening a json file\n",
        "# and initializing a variable.\n",
        "with urllib.request.urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
        "    counties = json.load(response)\n",
        "\n",
        "# We read in a data-frame from the web, specifying the data-types of various columns.\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv', dtype={\"fips\": str, \"state\": str, \"county\": str})\n",
        "# There is one \"fips\" code that needs to be corrected.\n",
        "df.loc[df['county'] == \"New York City\",'fips'] = \"36061\"\n",
        "# Here, we take the sum of each remaining variable in the set after using\n",
        "# \"fips\", \"state\", and \"county\" as grouping variables.\n",
        "df = df.groupby(['fips', 'state', 'county']).sum()\n",
        "# Reset index is simply a way to clear any index that was previously set...\n",
        "# This is useful sometimes in cleaning up after certain operations.\n",
        "df = df.reset_index()\n",
        "# Create a new variable that is a log (base 10) transform of the number of deaths.\n",
        "df['Deaths (log10)'] = np.log10(df['deaths'])\n",
        "print(df.head)\n",
        "\n",
        "# Here's where some magic comes in...\n",
        "# the counties file specified geometries / polygons (i.e. bounding boxes) \n",
        "# for each county. We use the data-frame describing deaths to colour our map.\n",
        "# The details of some of these methods are beyond the scope of the course,\n",
        "# but we show the example for completeness and inspiration!\n",
        "fig = px.choropleth(df, locations='fips',\n",
        "                        color='Deaths (log10)',\n",
        "                        scope='usa',\n",
        "                        geojson=counties,\n",
        "                        hover_data=['deaths'])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxWpoBMtgeM-"
      },
      "source": [
        "# Bikes in Montreal\n",
        "\n",
        "Link to data: https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/bikes.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmnybd3Oggme"
      },
      "source": [
        "# (1)\n",
        "# TODO: Properly read bikes.csv. \n",
        "# - Use ; as a separator\n",
        "# - Parse the column 'Date' as dates and note that the day comes first in the CSV (\"non-US\" way)\n",
        "# Checkout https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "# Yes it's a little scary, it would not be Pandas otherwise :-)\n",
        "# Search \"dayfirst\" and you'll find the option\n",
        "# Print the first 15 and check what you just read"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTtm6PU_cvEU"
      },
      "source": [
        "#@title Solution\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/bikes.csv',sep=';',parse_dates=[\"Date\"],dayfirst=True)\n",
        "df = df.set_index('Date')\n",
        "print(df.head(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQWfs7j-hOt"
      },
      "source": [
        "# (2)\n",
        "# TODO: Plot the number of bikes in \"du Parc\" as a function of time\n",
        "# Label the axes and put a title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RjufntVczDw"
      },
      "source": [
        "#@title Line-plot Solution\n",
        "df.plot(y='du Parc')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of bikes')\n",
        "plt.title('Du Parc\\'s bikes');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKStiJoK-b-"
      },
      "source": [
        "# (3)\n",
        "# TODO (bonus): Can you try to smooth out the curve ? \n",
        "# Use a 1-week moving average.\n",
        "# You'll have to google that.\n",
        "# Suggestions on keywords to use: \"pandas rolling average\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d214QiA72bDn"
      },
      "source": [
        "#@title Rolling Average Line-plot Solution\n",
        "df.rolling(7).mean().plot(y=['du Parc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vY0-BUND1T2"
      },
      "source": [
        "# (4) \n",
        "# TODO (bonus 2)\n",
        "#  1. Create a column holding the weekday\n",
        "#     df.index.weekday will give you that column.\n",
        "#  2. Sum all cyclists in each neighborhood for each week day\n",
        "#     Make a bar plot of the cyclists/day for each neighborhood\n",
        "#  3. Sum all neighborhood and make a pie chart of the total number\n",
        "#     of cyclists/day\n",
        "# Hint: \n",
        "# - df.index.weekday returns the weekday for each date in the index\n",
        "# - df.sum(axis=...) sums accross rows (axis = 0) or columns (axis = 1)\n",
        "# - df.plot(kind=...) can do bar plots (kind = bar) or pie (kind = pie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcHQxNxqykVw"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoOasbgXc3kp"
      },
      "source": [
        "#@title Bar-plot Solution\n",
        "# Clever: create a variable using the index. Since it's a date object, it has \n",
        "# a weekday index associated with it [0-6].\n",
        "df['Weekday'] = df.index.weekday\n",
        "# Here we calculate the sum of each column for each weekday.\n",
        "df_per_week = df.groupby('Weekday').sum()\n",
        "print(df_per_week)\n",
        "# Bar-plot (not necessarily the most interpretable or easy to read here, but\n",
        "# we show it for variety-sake...)\n",
        "df_per_week.plot(kind='bar')\n",
        "plt.title('Cyclists per day of the week per neighborhood')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZmfCFyGz3zj"
      },
      "source": [
        "#@title Pie-chart Solution\n",
        "# Here we calculate row-sums (where in the above Data.Frame each row was a weekday index)\n",
        "# I.e. the result is the sum *across neighborhoods* for each day of the week.\n",
        "df_per_week_all = df_per_week.sum(axis=1)\n",
        "# Again, as a DS I would never recommend using a pie-chart (an ordered barplot\n",
        "# would actually work better in this case...)\n",
        "df_per_week_all.plot(kind='pie')\n",
        "plt.title('Cyclists per day of the week')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YSxL9FSBz-j"
      },
      "source": [
        "# Extra Exercice: 311 Customer complaints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7APrVLMTua"
      },
      "source": [
        "Link to data: https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/311.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNGqHQWY8-40"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i811pYB49DK2"
      },
      "source": [
        "# TODO: Read the data, get a sense of what's in it by displaying some rows, printing columns names, etc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2VSAZKJdH-4"
      },
      "source": [
        "#@title Solution\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/asantucci/Python-Workshop/main/data/311.csv')\n",
        "print(df.head(5))\n",
        "print(df.describe())\n",
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3CgGXasCdaP"
      },
      "source": [
        "## Most common complaints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP3xVO0PAW1B"
      },
      "source": [
        "# TODO: Find the 10 most common complaint and visualize the distribution of complaints\n",
        "# Hint:\n",
        "# - df[column].value_counts() can count the number of occurences of entries in a column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVqp5pELdMKg"
      },
      "source": [
        "#@title Solution\n",
        "ct = df['Complaint Type'].value_counts()\n",
        "print(ct.head(10))\n",
        "ct.plot(kind='pie')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqQlbScNCfrN"
      },
      "source": [
        "## Plumbing complaints per borough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGxcCeC2BSyg"
      },
      "source": [
        "# TODO: Find the borough with the most PLUMBING complaints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gikLu78ddQeS"
      },
      "source": [
        "#@title Solution\n",
        "df_p = df[df['Complaint Type'] == 'PLUMBING']\n",
        "df_p_vc = df_p['Borough'].value_counts()\n",
        "df_p_vc.plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsebVy8GKiyW"
      },
      "source": [
        "## Time of complaint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA8BWPoqKl4J"
      },
      "source": [
        "# (1) 'Created Date' is a string in df.\n",
        "# Convert it to a proper DatetimeIndex\n",
        "# and keep the hour only\n",
        "# Tip: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html\n",
        "\n",
        "# (2) Count each occurance and plot the distribution\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK4tYh-HdUt8"
      },
      "source": [
        "#@title Solution\n",
        "hour = pd.DatetimeIndex(df['Created Date']).hour\n",
        "\n",
        "hour_count = hour.value_counts()\n",
        "hour_count.plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzddkW_RCmL0"
      },
      "source": [
        "## Harder: Analyse complaints geographical distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6aDXuxDDE7L"
      },
      "source": [
        "# TODO: Plot the position of the complaints\n",
        "# Tip: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H80SP7RKdboc"
      },
      "source": [
        "#@title Solution\n",
        "df.plot(kind='scatter',x='Longitude', y='Latitude')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESNcoTN5GMGj"
      },
      "source": [
        "# (2) TODO: Plot the position of the complaints registered in Manhattan only\n",
        "\n",
        "# (3) Round the longitude and latitude to the nearest 0.01\n",
        "# Tip: np.around should be helpful\n",
        "\n",
        "# (4) Group data by (latitude, longitude), count the size of each group\n",
        "# and aggregate\n",
        "\n",
        "# (5) Plot the (longitude, latitude, size) on a xy scatter plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj42HE4_dg-0"
      },
      "source": [
        "#@title Solution (2)\n",
        "# (2)\n",
        "man = df['Borough'] == 'MANHATTAN' \n",
        "df[man].plot(kind='scatter',x='Longitude', y='Latitude')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xAtcKbwWwh8"
      },
      "source": [
        "#@title Solution (3-4)\n",
        "df.loc[man,'Longitude'] = np.around(df.loc[man,'Longitude'], 2)\n",
        "df.loc[man,'Latitude'] = np.around(df.loc[man,'Latitude'], 2)\n",
        "s = df.loc[man,:].groupby(['Latitude','Longitude']).size()\n",
        "print(s.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0fAOb3F0a1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}